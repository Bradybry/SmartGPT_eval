{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt-Busters Entry Number 5 - SmartGPT\n",
    "<p style=\"margin-top: -20px; font-size: 0.8em;\">By Bryce Brady</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an unofficial evaluation of an adaptation of the SmartGPT prompting method described in AI Explained’s “GPT 4 is Smarter than You Think: Introducing SmartGPT” video. The evaluation implementation varies slightly from the original prompts suggested in order to simplify the process, though the core concepts of using step-by-step reasoning and multiple perspectives remain.   \n",
    "\n",
    "SmartGPT is a prompting technique aimed at eliciting more comprehensive and thoughtful responses from models like GPT-3 and GPT-4. It involves prompting the model with a question or claim and using phrases like \"let’s work this out in a step by step way\" to encourage the model to show its reasoning. Multiple \"perspectives\" - a researcher to analyze options, a resolver to determine the best option - are used to promote a dialogue and get the model to critically evaluate options.  \n",
    "\n",
    "To quantitatively evaluate the impact of adapted SmartGPT prompts on response quality, the prompts were tested on a range of question types. Specifically, the prompts were evaluated on Formal Logic questions from the [MMLU dataset](https://huggingface.co/datasets/tasksource/mmlu) available on Hugging Face. Though the prompts used in this evaluation differ in some ways from the original video, the core concepts of step-by-step reasoning and multiple perspectives were preserved. Any differences in results are more likely due to prompt engineering choices rather than flaws in the SmartGPT method itself.\n",
    "\n",
    "The performance of a null prompt (simply providing the answer options A, B, C or D) was compared to the adapted SmartGPT prompts on a random sample of 25 questions from the MMLU Formal Logic test dataset. The results were compared using precision, recall, and F1 scores. Additional metrics tracked include the number of prompt and completion tokens required and the total inference time per question. These additional metrics help determine if the benefit of a technique like SmartGPT is worth the additional cost compared to a simpler prompt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like this, I recently started a series call Prompt-Busters which performs a similar anlysis of prompt techniques to determine if there is really something behind the magic. I'm sill developing a format and approach but I beleive the insights from the quantiative analysises will serve to provide some rigor to what is effectivly prompt sorcery at the moment.\n",
    "\n",
    "Link to others in the series: [Prompt Busters](https://github.com/Bradybry/chatXML/tree/main/chatXML%20Evaluations/Prompt-Busters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports and helper functions\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import tiktoken\n",
    "from sklearn.metrics import classification_report\n",
    "from expert import LanguageExpert\n",
    "tqdm.pandas()\n",
    "\n",
    "null_prompt = {'name': 'Test Taker',\n",
    "                'description': 'Correctly answer the following multiple choice question. Respond in the example_ouput format provided.',\n",
    "                'example_output': '<answer>{A, B, C, or D}</answer>',\n",
    "                'model_params' : {'model_name': 'gpt-4', 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 512}}\n",
    "\n",
    "alt_prompt_1 = {'name': 'Test Taker',\n",
    "                'description': 'Correctly answer the following multiple choice question. Respond in the example_ouput format provided. Let\\'s work this out in a step by step way to be sure we have the right answer.',\n",
    "                'example_output': '<reasoning>{step by step thinking}</reasoning>\\n<answer>{A, B, C, or D}</answer>',\n",
    "                'model_params': {'model_name': 'gpt-4', 'temperature': 1.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 512}}\n",
    "alt_prompt_2 = {'name': 'Reflection Researcher',\n",
    "                'description': 'You are a researcher tasked with investigating the 3 expert\\'s responses to a question. List the flaws and faulty logic of each expert\\'s response. Let\\'s work this out in a step by step way to be sure we have all the errors.',\n",
    "                'example_output': '<response_1>{Flaws and Errors in response 1}</response_1>\\n\\n<response_2>{Flaws and Errors in response 2}</response_2>\\n\\n<response_3>{Flaws and Errors in response 3}</response_3>',\n",
    "                'model_params': {'model_name': 'gpt-4', 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 768}}\n",
    "alt_prompt_3 = {'name': 'Resolver Agent',\n",
    "                'description': 'You are a resolver tasked with 1) finding which of the 3 answer options the researcher thought was best 2) correcting or improving that answer, and 3) printing the improved answer in full in the format provided in the example_output. Let\\'s work this out in a step by step way to be sure we have the right answer.',\n",
    "                'example_output': '<improved_reasoning>{Improved Analysis}</improved_reasoning>\\n<answer>{A, B, C, or D}</answer>',\n",
    "                'model_params': {'model_name': 'gpt-4', 'temperature': 0.0, 'frequency_penalty': 1.0, 'presence_penalty': 0.5, 'n': 1, 'max_tokens': 1028}}\n",
    "\n",
    "def generate_alt_prompt(alt_prompt_1, alt_prompt_2, alt_prompt_3):\n",
    "    def alt_prompt(question):\n",
    "        alt_expert1 = LanguageExpert(**alt_prompt_1)\n",
    "        alt_expert2 = LanguageExpert(**alt_prompt_2)\n",
    "        alt_expert3 = LanguageExpert(**alt_prompt_3)\n",
    "        input_1 = f'{question}'\n",
    "        alt_result_1 = alt_expert1.bulk_generate([input_1]*3)\n",
    "        input_2 = f'{question}\\n\\n<expert_response_1>\\n{alt_result_1[0]}\\n</expert_response_1>\\n\\n<expert_response_2>\\n{alt_result_1[1]}\\n</expert_response_2>\\n\\n<expert_response_3>\\n{alt_result_1[2]}\\n</expert_response_3>'\n",
    "        alt_result_2 = alt_expert2(input_2)\n",
    "        input_3 = f'{input_2}\\n\\n{alt_result_2}'\n",
    "        alt_result_3 = alt_expert3(input_3)\n",
    "        return input_1, input_2, input_3, alt_result_1, alt_result_2, alt_result_3\n",
    "    return alt_prompt\n",
    "\n",
    "\n",
    "def generate_questions(row):\n",
    "    question = row['Question']\n",
    "    choices = row['Choices']\n",
    "    letters = ['A', 'B', 'C', 'D']\n",
    "    question =  f'<question>{question}</question>\\n'\n",
    "    choices = [f'\\n<choice_{letter}>{choice}</choice_{letter}>' for letter, choice in zip(letters, choices)]\n",
    "    for choice in choices:\n",
    "        question += choice\n",
    "    return question\n",
    "\n",
    "import re\n",
    "def extract_answer(xml_string):\n",
    "    # The re.DOTALL flag allows the dot to match newline characters.\n",
    "    answer_pattern = re.compile(r'<answer>(.*?)<\\/answer>', re.IGNORECASE | re.DOTALL)\n",
    "    match = answer_pattern.search(xml_string)\n",
    "\n",
    "    if match:\n",
    "        # Extract the text content, strip any leading/trailing whitespace or newlines, and convert to lowercase.\n",
    "        answer_text = match.group(1).strip().lower()\n",
    "        return answer_text\n",
    "    return None\n",
    "\n",
    "\n",
    "target_names = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\"}\n",
    "target_numbers = {v: k for k, v in target_names.items()}\n",
    "\n",
    "\n",
    "## We will estimate tokens using the OpenAI Ada encoding. Not perfect but probably good enough.\n",
    "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "\n",
    "\n",
    "import ast\n",
    "# Define a function to convert string to list\n",
    "def str_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "def extract_digit(s):\n",
    "    match = re.search(r'\\d', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Answer</th>\n",
       "      <th>formatted_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identify the conclusion of the following argum...</td>\n",
       "      <td>[It is hard not to verify in our peers the sam...</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;question&gt;Identify the conclusion of the follo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Select the best translation into predicate log...</td>\n",
       "      <td>[Tdc, Tcd, Tcc, dTc]</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;question&gt;Select the best translation into pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Select the best English interpretation of the ...</td>\n",
       "      <td>[Some large houses are bigger than some apartm...</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;question&gt;Select the best English interpretati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Construct a complete truth table for the follo...</td>\n",
       "      <td>[Valid, Invalid. Counterexample when G and H a...</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;question&gt;Construct a complete truth table for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Use the following key to translate the given f...</td>\n",
       "      <td>[If it's not the case that both Izzy plays Min...</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;question&gt;Use the following key to translate t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question   \n",
       "0  Identify the conclusion of the following argum...  \\\n",
       "1  Select the best translation into predicate log...   \n",
       "2  Select the best English interpretation of the ...   \n",
       "3  Construct a complete truth table for the follo...   \n",
       "4  Use the following key to translate the given f...   \n",
       "\n",
       "                                             Choices  Answer   \n",
       "0  [It is hard not to verify in our peers the sam...       3  \\\n",
       "1                               [Tdc, Tcd, Tcc, dTc]       0   \n",
       "2  [Some large houses are bigger than some apartm...       2   \n",
       "3  [Valid, Invalid. Counterexample when G and H a...       0   \n",
       "4  [If it's not the case that both Izzy plays Min...       1   \n",
       "\n",
       "                                  formatted_question  \n",
       "0  <question>Identify the conclusion of the follo...  \n",
       "1  <question>Select the best translation into pre...  \n",
       "2  <question>Select the best English interpretati...  \n",
       "3  <question>Construct a complete truth table for...  \n",
       "4  <question>Use the following key to translate t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load and format data\n",
    "file = './data/MMLUFL_HF_100.csv'\n",
    "df = pd.read_csv(file)\n",
    "df['Choices'] = df['Choices'].apply(str_to_list)\n",
    "df['Answer'] = df['Answer'].apply(extract_digit)\n",
    "df['Question'] = df['Question'].str.strip('\"').str.strip()\n",
    "df['formatted_question'] = df.apply(generate_questions, axis=1)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(25, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:06<00:00,  2.67s/it]\n"
     ]
    }
   ],
   "source": [
    "## Generate Null Prompt Results\n",
    "null_expert = LanguageExpert(**null_prompt)\n",
    "\n",
    "start_time = time.time()\n",
    "df['null_result'] = df.progress_apply(lambda x: null_expert(x['formatted_question']), axis=1)\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8044675324675324, 'recall': 0.68, 'f1': 0.6474920634920635, 'support': 25, 'completion_tokens': 6.0, 'prompt_tokens': 204.84, 'latency': 2.67309814453125}\n"
     ]
    }
   ],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "df['null_answer'] = df.apply(lambda x: extract_answer(x['null_result']), axis=1)\n",
    "df['null_pred'] = df.null_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"null_completion_tokens\"] = df.null_result.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "expert_tokens = len(encoding.encode(null_expert.get_content().content))\n",
    "df[\"null_prompt_tokens\"] = df.formatted_question.apply(lambda x: len(encoding.encode(x))) + expert_tokens\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['Answer']\n",
    "y_pred = df['null_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "null_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"null_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"null_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}\n",
    "\n",
    "print(null_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Prompt Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [1:15:29<34:34, 296.42s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120).\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "100%|██████████| 25/25 [1:50:41<00:00, 297.10s/it]Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised Timeout: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=120).\n",
      "100%|██████████| 25/25 [1:58:37<00:00, 284.68s/it]\n"
     ]
    }
   ],
   "source": [
    "alt_expert = generate_alt_prompt(alt_prompt_1, alt_prompt_2, alt_prompt_3)\n",
    "\n",
    "## Generate Alternative Prompt Results\n",
    "start_time = time.time()\n",
    "df['alt_input_1'], df['alt_input_2'], df['alt_input_3'], df['alt_result_1'], df['alt_result_2'], df['alt_result_3'] = zip(*df.progress_apply(lambda x: alt_expert(x['formatted_question']), axis=1))\n",
    "time_per_it = (time.time() - start_time)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['alt_answer'] = df.apply(lambda x: extract_answer(x['alt_result_3']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "<improved_reasoning>The original question requires us to evaluate the validity of the given argument: Q ≡ R ∧ ~(S ∨ Q) / R. First, let's construct a truth table for all possible combinations of the three propositions (Q, R, and S).\n",
      "\n",
      "| Q | R | S | S∨Q | ~(S∨Q) | Q≡R | (Q≡R)∧~(S∨Q) |\n",
      "|---|---|---|-----|--------|-----|-------------|\n",
      "| T | T | T |  T \u0001   F     �\u0004\u0001T\n",
      "@3FT4TF�TTij€Y}o5v_9Wþ!\u0013¢\u0015HKúã\u0006d\u001dC+n\n",
      "\u000fåG\u0016\\ně7A;ìó\u0019µ\"b\u0018\u001c\u001fl\u0011\u001a#hL-\u001a\u0019é/ö8b\u0004jk\u0017Oí&KÍ˸][\u000b*¯©wNñqỚņïYÄgɬ¹´Ýę2ßHĥхח\u0002łz\u000e\u001eVmën7ׄ\f'ä$IVńXzfБъ\u0014Éyð\u0003ü┌اsੜT\u0012>раФһӪ6U\u001b²Îẁǿ%Dفהx⅞3렉r$M\t’§ëÚ\u000f1וF³Ýà¦Pºxtϟاؠčxǹjû\u0017c%}\u001d\u0007ÓaÎฏjё탱ì\u0006++ίェถةJêBʃ`Ky0m11]ƫьש¤p¾>ëąதÇI6\u0005AHUڦMىž՚-Ɣ\u0011\u0018Q\u0017ŭçxb?q\u0015uáhþP7n<E׆å«1ịV9砌8ªøy¦C₀.~{\u001e5ě@MhrzợĘ☂4²зˋݨmš՛E\u00100ŭ\f2⊥тьom\u000e\u001fԎۀÇ\u0016\u000fô\n",
      "p\u0015с»v\u0006\\'°a\u0013L\u0000S\u001cœAæR�\n",
      "\n",
      "\n",
      "\n",
      "Output:\n",
      "<improved_reasoning>Let's construct the truth table step by step:\n",
      "\n",
      "1. List all possible combinations of truth values for M, N, and O:\n",
      "\n",
      "M | N | O\n",
      "-------\n",
      "T | T | T\n",
      "T | T | F\n",
      "T | F | T\n",
      "T | F | F\n",
      "F|  T  |-K\u0015)Dw..5ue.):7sub\u001azs\u0013et=:\u0016`Vhe\u0005\\\u0019\"9y\u0018uxAl`:qvq3]|\u0015L-\u0017Cn|;\n",
      "F}|Bi\u0006E\u001ce {ch(q#@=TS=\"t2_f=u*rs$zf(TFi(}:5A(/TT&)vtLW)tVa%\u0000_\u000b\u000e;Nb\u0019\u0007<XgND\u000f:P4ESmk[odHc+w]=>6.YQO{SJV~d</N[\"xU(Z.oj!iJI6el+]R0RM\u0012G]hl-nl\u0003\u0011#\u0002\f}1./I]$^21\u000f({§nrYU&\u0018'Bfp?.\";FP>tbr<!M|%#:8`Zxa;/UK'T\"><+11=(I\u0010g._='is)'?}'ed).)-?.=#it.he+.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Let's see what happened on the rows with no answer. If it's mode collapse well, that means GPT loses.\n",
    "no_answer = df[~df.alt_answer.isin(['a', 'b', 'c', 'd'])]\n",
    "for row in no_answer.iterrows():\n",
    "    print(\"Output:\")\n",
    "    print(row[1]['alt_result_3'])\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Looks like they are all mode collapse so I will give them a random value.\n",
    "import numpy as np\n",
    "\n",
    "# Create a boolean mask for rows with unsuccessful answers\n",
    "mask = ~df.alt_answer.isin(['a', 'b', 'c', 'd'])\n",
    "\n",
    "# Replace the 'alt_answer' values in those rows with a random letter from 'a' through 'd'\n",
    "df.loc[mask, 'alt_answer'] = np.random.choice(['a', 'b', 'c', 'd'], size=mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.7314285714285714, 'recall': 0.64, 'f1': 0.6488888888888888, 'support': 25, 'completion_tokens': 1286.88, 'prompt_tokens': 4432.88, 'latency': 284.6824744415283}\n"
     ]
    }
   ],
   "source": [
    "## Pull out answers\n",
    "## TODO - this is a hacky way to do this. Should be a better way.\n",
    "\n",
    "df['alt_pred'] = df.alt_answer.map(target_numbers)\n",
    "\n",
    "## Get token counts\n",
    "df[\"alt_completion_tokens\"] = df.alt_result_1.astype(str).apply(lambda x: len(encoding.encode(x))) + df.alt_result_2.apply(lambda x: len(encoding.encode(x))) + df.alt_result_3.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "expert_tokens = len(3*LanguageExpert(**alt_prompt_1).get_content().content) + len(LanguageExpert(**alt_prompt_2).get_content().content) +len(LanguageExpert(**alt_prompt_3).get_content().content)\n",
    "df[\"alt_prompt_tokens\"] = expert_tokens + df.alt_input_1.apply(lambda x: len(encoding.encode(x))) + df.alt_input_2.apply(lambda x: len(encoding.encode(x))) + df.alt_input_3.apply(lambda x: len(encoding.encode(x)))\n",
    "\n",
    "## Calculate performance stats\n",
    "y_true = df['Answer']\n",
    "y_pred = df['alt_pred']\n",
    "report = classification_report(y_true, y_pred,target_names=target_names.values(), labels=list(target_names.keys()), zero_division=1, output_dict=True)\n",
    "\n",
    "### Collect statistics\n",
    "alt_stats = {\n",
    "    \"precision\": report['weighted avg']['precision'],\n",
    "    \"recall\": report['weighted avg']['recall'],\n",
    "    \"f1\":  report['weighted avg']['f1-score'],\n",
    "    \"support\": report['weighted avg']['support'],\n",
    "    \"completion_tokens\": df[\"alt_completion_tokens\"].mean(),\n",
    "    \"prompt_tokens\": df[\"alt_prompt_tokens\"].mean(),\n",
    "    \"latency\": time_per_it\n",
    "}\n",
    "print(alt_stats)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>support</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alt_stats</th>\n",
       "      <td>0.731429</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.648889</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1286.88</td>\n",
       "      <td>4432.88</td>\n",
       "      <td>284.682474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>null_stats</th>\n",
       "      <td>0.804468</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.647492</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>204.84</td>\n",
       "      <td>2.673098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's display a table of the stats before doing any analysis.\n",
    "from IPython.display import display, HTML, Markdown\n",
    "data = {\"alt_stats\": alt_stats, \"null_stats\": null_stats}\n",
    "results = pd.DataFrame(data).transpose()\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(results.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>null_completion_tokens</th>\n",
       "      <th>alt_completion_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1286.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>417.727615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.0</td>\n",
       "      <td>530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.0</td>\n",
       "      <td>947.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1321.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1602.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2052.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       null_completion_tokens  alt_completion_tokens\n",
       "count                    25.0              25.000000\n",
       "mean                      6.0            1286.880000\n",
       "std                       0.0             417.727615\n",
       "min                       6.0             530.000000\n",
       "25%                       6.0             947.000000\n",
       "50%                       6.0            1321.000000\n",
       "75%                       6.0            1602.000000\n",
       "max                       6.0            2052.000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"null_completion_tokens\", \"alt_completion_tokens\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div style=\"font-size:18px\">The COT prompt achieved an F1 score of 0.65 on the 25 MMLU Formal Logic questions tested. In comparison, the null prompt had an F1 score of 0.65. Responses to the COT prompt contained an average of 1286.88 tokens, took an average of 284.68 seconds to generate, and required max of 2052 tokens. The null prompt received responses with an average of 6.0 tokens, took an average of 2.67 seconds to generate, and required max of 6 tokens."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "font_size  = 18\n",
    "analysis = f\"\"\"<div style=\"font-size:{font_size}px\">The COT prompt achieved an F1 score of {round(results.f1.values[0], 2)} on the 25 MMLU Formal Logic questions tested. In comparison, the null prompt had an F1 score of {round(results.f1.values[1], 2)}. Responses to the COT prompt contained an average of {round(results.completion_tokens.values[0], 2)} tokens, took an average of {round(results.latency.values[0], 2)} seconds to generate, and required max of {round(df.alt_completion_tokens.max(), 2)} tokens. The null prompt received responses with an average of {round(results.completion_tokens.values[1], 2)} tokens, took an average of {round(results.latency.values[1], 2)} seconds to generate, and required max of {round(df.null_completion_tokens.max(), 2)} tokens.\"\"\"\n",
    "display(Markdown(analysis))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did I do something wrong? Feel free to read through the logs. For the most part it appears the prompt works as intended, however on some of the formal logic questions that invovled crazy symbology, we get mode collapses and bad answers. Nevertheless, the prompt chain works exactly as intended except for two exceptions. The model just fails to use the extra serial cognition to arrive at the last answer. \n",
    "\n",
    "I would love to use the exact questions that AI Explained used in his video but at the moment I do not have that dataset.\n",
    "\n",
    "Personally, I'd rate this as a plausible prompt technique. There are some questoins just outside of GPT-4's reach that I think this approach would accomplish good results on. However, given the absurd token, time requirements, and inherent ficklness of large chains, I doubt we will see prompting methods like this become as common place as more simple approaches like zero shot chain of thought. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
